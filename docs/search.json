[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Dylan. I’m a second year Computer Science student at Trinity College Dublin. This is my blog where I share some of the stuff that I’m working on."
  },
  {
    "objectID": "posts/nascar-f1-classifier/NASCAR_F1_Classifier.html",
    "href": "posts/nascar-f1-classifier/NASCAR_F1_Classifier.html",
    "title": "NASCAR or F1 Car Classifier",
    "section": "",
    "text": "!pip install -Uqq fastai duckduckgo_search\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.4/75.4 kB 2.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 74.5/74.5 kB 9.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 6.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.7/2.7 MB 39.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 6.4 MB/s eta 0:00:00\n\n\n\nfrom duckduckgo_search import ddg_images\nfrom fastcore.all import *\n\ndef search_images(term, max_images=30):\n    print(f\"Searching for '{term}'\")\n    return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\n\nurls = search_images('nascar photos', max_images=1)\nurls[0]\n\nSearching for 'nascar photos'\n\n\n/usr/local/lib/python3.10/dist-packages/duckduckgo_search/compat.py:60: UserWarning: ddg_images is deprecated. Use DDGS().images() generator\n  warnings.warn(\"ddg_images is deprecated. Use DDGS().images() generator\")\n/usr/local/lib/python3.10/dist-packages/duckduckgo_search/compat.py:64: UserWarning: parameter page is deprecated\n  warnings.warn(\"parameter page is deprecated\")\n/usr/local/lib/python3.10/dist-packages/duckduckgo_search/compat.py:66: UserWarning: parameter max_results is deprecated\n  warnings.warn(\"parameter max_results is deprecated\")\n\n\n'https://heavy.com/wp-content/uploads/2020/11/GettyImages-1283397584-e1604803312517.jpg?quality=65&strip=all'\n\n\n\nfrom fastdownload import download_url\ndest = 'nascar.jpg'\ndownload_url(urls[0], dest, show_progress=False)\n\nfrom fastai.vision.all import *\nim = Image.open(dest)\nim.to_thumb(256,256)\n\n\n\n\n\ndownload_url(search_images('f1 photos', max_images=1)[0], 'f1.jpg', show_progress=False)\nImage.open('f1.jpg').to_thumb(256,256)\n\nSearching for 'f1 photos'\n\n\n/usr/local/lib/python3.10/dist-packages/duckduckgo_search/compat.py:60: UserWarning: ddg_images is deprecated. Use DDGS().images() generator\n  warnings.warn(\"ddg_images is deprecated. Use DDGS().images() generator\")\n/usr/local/lib/python3.10/dist-packages/duckduckgo_search/compat.py:64: UserWarning: parameter page is deprecated\n  warnings.warn(\"parameter page is deprecated\")\n/usr/local/lib/python3.10/dist-packages/duckduckgo_search/compat.py:66: UserWarning: parameter max_results is deprecated\n  warnings.warn(\"parameter max_results is deprecated\")\n\n\n\n\n\n\nsearches = 'f1', 'nascar'\npath = Path('race_series')\nfrom time import sleep\n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f'{o} photo'))\n    sleep(10)  # Pause between searches to avoid over-loading server\n    download_images(dest, urls=search_images(f'{o} old photo'))\n    sleep(10)\n    resize_images(path/o, max_size=400, dest=path/o)\n\nSearching for 'f1 photo'\nSearching for 'f1 old photo'\nSearching for 'nascar photo'\nSearching for 'nascar old photo'\n\n\n\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n2\n\n\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path, bs=32)\n\ndls.show_batch(max_n=6)\n\n\n\n\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(3)\n\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00&lt;00:00, 71.4MB/s]\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.431362\n0.930054\n0.300000\n00:09\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.660739\n0.551799\n0.250000\n00:13\n\n\n1\n0.482893\n0.117785\n0.000000\n00:14\n\n\n2\n0.360152\n0.058476\n0.000000\n00:13\n\n\n\n\n\n\nis_f1,_,probs = learn.predict(PILImage.create('f1.jpg'))\nprint(f\"This is a: {is_f1}.\")\nprint(f\"Probability it's a f1 car: {probs[0]:.4f}\")\n\n\n\n\n\n\n\n\nThis is a: f1.\nProbability it's a f1 car: 0.9282\n\n\n\nlearn.export(\"model.pkl\")"
  },
  {
    "objectID": "posts/cnn-notes/cnn-notes.html",
    "href": "posts/cnn-notes/cnn-notes.html",
    "title": "CNN Notes",
    "section": "",
    "text": "I have been working through Andrew Ng’s Deep Learning Specialisation. Here are some of the notes I took during the Convolutional Neural Networks Course. Hopefully they can be of use to you. Some images are taken from the slides in the course."
  },
  {
    "objectID": "posts/cnn-notes/cnn-notes.html#notes",
    "href": "posts/cnn-notes/cnn-notes.html#notes",
    "title": "CNN Notes",
    "section": "Notes",
    "text": "Notes\n\nFoundations of Convolutional Neural Networks\n\nVertical Edge Detection\nYou have a filter and you ‘convolve’ it with the input image You do an element-wise product with the filter overlaying the input image in different positions. The filter for vertical edge detection typically looks something like this \\[\n\\begin{bmatrix}\n1 & 0 & -1 \\\\\n1 & 0 & -1 \\\\\n1 & 0 & -1 \\\\\n\\end{bmatrix}\n\\]\n\n\nPositive and Negative Vertical Edges\nDistinguishes between light-to-dark and dark-to-light edges\n\n\nLearning to Detect Edges\nModern DL does not explicitly define the filter, but rather sets each of the elements as a weight/parameter that the model can learn itself.\n\n\nPadding\nNormal convolutions have some problems. They do not use much information from the edge pixels and they also shrink the output. You can get around both of these problems by padding the border of the input with zeroes.\n\n\nSize of output\n\\((n+2p-f+1) \\times (n+2p-f+1)\\)\n\n\nValid Convolutions\nNo padding #### Same Convoultions Output size is the same as the input size. You can achieve this if you set \\(f\\) and \\(p\\) to be \\[p = \\frac{f-1}{2}\\] To make this formula work, \\(f\\) is normally odd. This has some advantages such as the filter having a central pixel.\n\n\nStrided Convolutions\nYou can add a ‘stride’ parameter \\(s\\) to the convolution. This means you ‘jump’ the filter over by \\(s\\) pixels each time. The new formula for computing the output size is \\[\\lfloor \\frac{n+2p-f}{s} + 1\\rfloor \\times \\lfloor \\frac{n+2p-f}{s} + 1\\rfloor \\] Where \\(\\lfloor x \\rfloor\\) denotes the ‘floor’ of the expression\n\n\nConvolutions over Volume\nYou can convolve a 3d input with a 3d filter. You put the 3d filter over the 3d input as you would in 2d, and you calculate the element-wise product and use that as the first cell of the output.\nNote: the input volume and filter need to have the same number of channels.\nThe model can learn to detect more complex patterns such as vertical edges in only the red channel for example.\n\n\nMultiple Filters\nYou can have multiple filters per layer If you have \\(n_c'\\) filters, then you will have \\(n_c'\\) outputs, which you stack to form a \\(\\lfloor \\frac{n+2p-f}{s} + 1\\rfloor \\times \\lfloor \\frac{n+2p-f}{s} + 1\\rfloor \\times n_c'\\) output\n\n\nOne Layer of a CNN\n\n\n\nimage.png\n\n\n\n\nMulti-Layer CNN\nAs you get deeper in the NN, you reduce height and width, while increasing the number of channels.\n\n\nPooling Layers: Max Pooling\nYou get the max of the corresponding region  Max pooling has hyperparameters \\(f\\) and \\(s\\), but it does not have any weights to learn.\nThe same formula for computing the size of the output still works here: \\[\\lfloor \\frac{n+2p-f}{s} + 1\\rfloor \\times \\lfloor \\frac{n+2p-f}{s} + 1\\rfloor \\times n_c\\]\n\n\nAverage Pooling\nThis is the same as max pooling, but taking the average of the region instead of the max.\nNote: padding is very rarely used in either max or average pooling\n\n\nConvolutional Neural Network Example\n Normally, we put one or more convolutional layer (CONV) followed by a pooling layer (POOL), and repeat this pattern multiple times.\nWhen we are done with the convolutional layers, we ‘flatten’ them out and pass them through a few fully connected (FC) layers.\n\n\nWhy Convolutions?\n\nParameter Sharing\nSparcity of Connections\n\nParameter sharing lets learned parameters to be used in multiple parts of the image. This drastically reduces the total number of parameters required in the neural network.\nSparcity of Connections: since every output is calculated on a small section of the input at a time, it will be easier to learn the parameters.\n\n\nPutting it all together\n\nDecide on an architecture\nDecide on a loss function \\(L\\)\nRandomly initialize all the weights and biases\nUse an optimizer (like gradient descent or Adam) to optimize the parameters\n\n\n\n\nMore Complicated CNNs\n\nResidual Blocks (ResNets)\nResidual blocks get around the vanishing/exploding gradients problem in very deep NNs.\nThey copy an activation from one layer and feed it into another layer deeper in the network.\n\n\n\nimage.png\n\n\n\\[a^{[l+2]} = g(z^{[l+2]} + a^{[l]})\\]\nIf you add more and more layers to a traditional FFNN, it can actually increase training error.\nAdding a ResNet layer doesn’t hurt training error because it is very easy for it to learn the identity function.\nOne thing to note, is that \\(a^{[l]}\\) and \\(a^{[l+2]}\\) have to have the same dimension for the addition to work, so a lot of ‘same’ convolutions are used for this reason.\n\n\n1x1 Convolutions\n1x1 convolutions are used to change the number of channels, \\(n_c\\). For example, if you have a \\(28 \\times 28 \\times 192\\) layer input, and want to reduce the number of channels to \\(32\\) then you pass is through a \\(1 \\times 1\\) convolution with \\(32\\) filters\n\n\nInception Layers\nInception layers can have multiple filter sizes and stack the outputs \nThis lets the model learn which of the filters are most useful and learns the correct parameters for them by itself.\nWe can reduce the computational cost of inceptions layers by using a \\(1 \\times 1\\) convolution to calculate the \\(5 \\times 5\\) convolution. \nSo long as you implement the bottleneck layer within reason, you retain a lot of the power of the original \\(5 \\times 5\\) convolution.\nWe can do the same for the \\(3 \\times 3\\) convolution.\nThe inception layer now looks like this. \nAlthough the maxpool layer gives the correct number of channels as output, it’s still significantly higher than the others, so we pass it through a \\(1 \\times 1\\) convolution with a smaller number of filters, for example \\(32\\) so it takes up a more reasonable amount of the total channels.\n\n\nDepthwise Separable Convolution\n You first pass the input into a Depthwise Convolution and then into a Pointwise Convoluion.\n The depthwise convolution works as follows, you convolve each of the input channels with only the corresponding filter channel. The outputs give the outputs of each of the channels of the output.\n For the pointwise convolution, you have \\(n_c'\\) filters of size \\(1 \\times 1 \\times n_c\\). The output is calculated one channel at a time, using the calculated convolution between the input and each of the filters.\nDepthwise separable convolutions tend to be a lot faster than normal convolutions. The ratio of computation time is given by\n\\[\\frac{1}{n_c'} + \\frac{1}{f^2}\\]\n\n\nImage Classification, Localization and Detection\n\n\n\nimage.png\n\n\nImage classification: Detecting presence or absense of an object\nClassification with localization: Detects presence or absence of object and gives its location\nDetection: Gives locations of multiple objects in an image.\n\n\nClassification with localization\nTo train a model to do classification with localization, your training data needs to have images as inputs and a vector \\(y\\) as the label.\n\\[ \\begin{bmatrix}\np_c \\\\\nb_x \\\\\nb_y \\\\\nb_h \\\\\nb_w \\\\\nc_1 \\\\\nc_2 \\\\\nc_3\n\\end{bmatrix}\n\\]\nThe loss function \\(L(\\hat{y}, y)\\) can be defined using the squared error: \\[\nL(\\hat{y}, y) =\n\\begin{cases}\n(y_{\\hat{1}} - y_1)^2 + (y_{\\hat{2}} - y_2)^2 + \\cdots + (y_{\\hat{8}} - y_8)^2 & \\text{if } y_1 = 1, \\\\\n(y_{\\hat{1}} - y_1)^2 & \\text{if } y_1 = 0.\n\\end{cases}\n\\]\n\n\nLandmark Detection\nThe above method can also be used for landmark detection. By selecting multiple landmarks and labelling them with (for example in people’s faces), the model can learn to do landmark detection on new, unseen faces."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dylan's Blog",
    "section": "",
    "text": "Best Maths Resources for Machine Learning\n\n\n\n\n\n\n\nBlog\n\n\n\n\n\n\n\n\n\n\n\nNov 3, 2023\n\n\nDylan Gallagher\n\n\n\n\n\n\n  \n\n\n\n\nCNN Notes\n\n\n\n\n\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n\nSep 9, 2023\n\n\nDylan Gallagher\n\n\n\n\n\n\n  \n\n\n\n\nNASCAR or F1 Car Classifier\n\n\n\n\n\n\n\nProject\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\nDylan Gallagher\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/maths-resources-for-ml/maths-resources-for-ml.html",
    "href": "posts/maths-resources-for-ml/maths-resources-for-ml.html",
    "title": "Best Maths Resources for Machine Learning",
    "section": "",
    "text": "Why learn maths anyway?\nI’ll start by saying that if you just want to use machine learning libraries, especially the more beginner-friendly ones like fast.ai, then you don’t really need to know much maths at all, it definitely helps, but it’s not at all necessary. If, on the other hand, if you want to dive a little deeper and learn how and why these algorithms work as well as they do, then taking the time to learn some of the maths is a worthwhile investment.\nBy far the most important areas of maths that machine learning is based on are: - Linear Algebra - Calculus - Probability and Statistics\nI’ll discuss these one-by-one and give you my recommended resources for each of them.\n\n\nLinear Algebra\nLinear Algebra is used a lot in machine learning. Data in machine learning models are represented by matrices. Once you learn about matrices, as well as fundamental operations on matrices such as matrix multiplication and the transpose of a matrix, you’ll find that a lot of dauntingly-complex concepts such as neural networks are nothing more than matrix multiplications. There are many other applications of linear algebra in machine learning, and these include Support Vector Machines and Principal Component Analysis, but we won’t go into them in this blog post.\nNow that I’ve hopefully convinced you of the importance of linear algebra, here are some great resources:\n\nKhan Academy: Linear Algebra\nKhan Academy is a staple in education and the Linear Algebra section is no exception. He explains everything very well and it’s relatively easy to follow. If you just want a quick treatment of linear algebra, this is the resource for you. There’s no practice problems, which is a real shame because that’s where I found a lot of my understanding came from. An introductory linear algebra textbook can solve this problem. I picked up an older edition of ‘Elementary Linear Algebra’ by Anton for not much on eBay, which had many practice problems. It also served as a useful reference for linear algebra.\n\n\n3Blue1Brown : The essence of linear algebra\n3Blue1Brown’s YouTube channel is aimed at the mathematically curious. His ‘essence of’ series are some of his best content in my opinion. This series of videos can be watched before, after, or during a more traditional linear algebra course. It doesn’t teach you everything there is to know about linear algebra, but that’s not its goal either. The videos are made up of intuitive visualisations that help you understand the fundamental operations beyond just the formulas.\n\n\nMIT OpenCourseWare: Linear Algebra\nIf you want to go sligtly deeper into linear algebra, then this is the resource for you. This is a full university treatment of linear algebra. The teacher, Gilbert Strang, is amazing, I cannot complement him enough. His lectures, alongside his book, provide some of the best explanations of linear algebra concepts I’ve seen anywhere. This course is quite tough, especially compared to the Khan Academy one, but if you complete it, you’ll have an excellent understanding of linear algebra and its applications.\n\n\nIntroduction to Linear Algebra - Gilbert Strang\nThis is the accompanying book to the MIT lectures. If you are watching those lectures, I highly recommend this book. It’s amazing. I find it much better than Anton’s book. It is a bit more expensive, even second hand, but it’s definitely worth it in my opinion. This book (and the accompanying lectures) are all you need to master linear algebra. The questions at the end of the chapter start manageable, but then get really tricky, which I think is a good thing.\n\n\n\nCalculus\nCalculus is another fundamental concept in machine learning. You can get by quite well with just a secondary-school-level understanding of calculus. As long as you know what a derivative is and what it represents, that’ll get you a long way. However, learning more advanced topics such as Multivariable Calculus is extremely helpful in the context of machine learning, as the functions we are working with are in high-dimensional space. If this sounds interesting to you, these resources will be very helpful.\n\nProfessor Leonard\nProfessor Leonard has an amazing series on YouTube covering everything from the basics of limits all the way up to multivariable calculus. These videos are recorded in a classroom and I found myself paying a lot more attention to this style of video compared to the electronic blackboard videos of Khan Academy (although they’re still great). He goes at a nice pace, and his explanations are amazing. He also does a lot of worked examples which helps a ton. The book which he follows is by Stewart. I started that book, but I wasn’t a huge fan of it, but then I found Thomas’ Calculus, which I’ll talk about now.\n\n\nThomas’ Calculus\nThis book, much like Stewart’s book is a fairly standard Calculus book. You’ll end up learning the same stuff from either but I much prefered the explanations in this book. It’s also readily available second hand on eBay for a lot cheaper. I used this book in conjunction with Professor Leonard’s videos. This combination will give you a comprehensive coverage of Calculus. The questions in the book are varied, some being more mechanical ‘plug-and-chug’ while others focused on applications.\n\n\n\nProbability and Statistics\nCovering Probability and Statistics will round up your maths foundation nicely, letting you focus on the ML-specific theory without having to jump back and forth between maths and ML. Probability and Statistics come up all the time in AI and Machine Learning. From simple classical ML models like linear and logistic regression all the way up to cutting-edge diffusion models, having a basic understanding of probability and statistics will help you immensely. So, without further ado, here’s my list of recommended resources:\n\nKhan Academy : Probability and Statistics\nKhan Academy saves the day again! This course on probability and statistics starts basically from scratch, which can’t be said for all the resources on this list. It assumes very little of the student and starts from the basics like the mean, median and mode, and ends up in relatively advanced statistical tools like ANOVA. This series should be all the probability and statistics you need for your machine learning journey, but that being said, if you’re interested in learning more, the next course on the list is very good.\n\n\nSTAT 110 - Harvard University\nI’ll start by saying this course is intense. Joe Blitzstein is exceptional at explaining the concepts, but there’s no getting round the fact that you’ll most likely struggle. There is a free book that goes with the course, written partly by Joe Blitzstein, which goes into more detail about the topics covered. The requirements for this course are hefty enough, you’ll need a really good working knowledge of sets as well as quite a bit of calculus. He recommends at least being familiar with multivariable calculus, which does help understand certain related topics in probability. Overall, it’s a great course. I recommend checking out the first few lectures to get a feel for the pace and difficulty and maybe come back to it after doing the Khan Academy course if it interests you.\nAnd that’s about it!\nNow I wouldn’t necessarily recommend learning all of this maths before diving into machine learning. Learn by doing! A lot of the concepts of machine learning become so much clearer when you actually implement them in code, so if you are following a machine learning course or book and get stuck, try coding it out first before jumping to the conclusion that you ‘need more maths’, because you probably don’t.\nAnyways, that’s been my two cents on the topic. I hope you enjoyed."
  }
]