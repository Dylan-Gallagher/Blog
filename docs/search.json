[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Dylan. I’m a second year Computer Science student at Trinity College Dublin. This is my blog where I share some of the stuff that I’m working on."
  },
  {
    "objectID": "posts/nascar-f1-classifier/NASCAR_F1_Classifier.html",
    "href": "posts/nascar-f1-classifier/NASCAR_F1_Classifier.html",
    "title": "NASCAR or F1 Car Classifier",
    "section": "",
    "text": "!pip install -Uqq fastai duckduckgo_search\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.4/75.4 kB 2.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 74.5/74.5 kB 9.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 6.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.7/2.7 MB 39.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 6.4 MB/s eta 0:00:00\n\n\n\nfrom duckduckgo_search import ddg_images\nfrom fastcore.all import *\n\ndef search_images(term, max_images=30):\n    print(f\"Searching for '{term}'\")\n    return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\n\nurls = search_images('nascar photos', max_images=1)\nurls[0]\n\nSearching for 'nascar photos'\n\n\n/usr/local/lib/python3.10/dist-packages/duckduckgo_search/compat.py:60: UserWarning: ddg_images is deprecated. Use DDGS().images() generator\n  warnings.warn(\"ddg_images is deprecated. Use DDGS().images() generator\")\n/usr/local/lib/python3.10/dist-packages/duckduckgo_search/compat.py:64: UserWarning: parameter page is deprecated\n  warnings.warn(\"parameter page is deprecated\")\n/usr/local/lib/python3.10/dist-packages/duckduckgo_search/compat.py:66: UserWarning: parameter max_results is deprecated\n  warnings.warn(\"parameter max_results is deprecated\")\n\n\n'https://heavy.com/wp-content/uploads/2020/11/GettyImages-1283397584-e1604803312517.jpg?quality=65&strip=all'\n\n\n\nfrom fastdownload import download_url\ndest = 'nascar.jpg'\ndownload_url(urls[0], dest, show_progress=False)\n\nfrom fastai.vision.all import *\nim = Image.open(dest)\nim.to_thumb(256,256)\n\n\n\n\n\ndownload_url(search_images('f1 photos', max_images=1)[0], 'f1.jpg', show_progress=False)\nImage.open('f1.jpg').to_thumb(256,256)\n\nSearching for 'f1 photos'\n\n\n/usr/local/lib/python3.10/dist-packages/duckduckgo_search/compat.py:60: UserWarning: ddg_images is deprecated. Use DDGS().images() generator\n  warnings.warn(\"ddg_images is deprecated. Use DDGS().images() generator\")\n/usr/local/lib/python3.10/dist-packages/duckduckgo_search/compat.py:64: UserWarning: parameter page is deprecated\n  warnings.warn(\"parameter page is deprecated\")\n/usr/local/lib/python3.10/dist-packages/duckduckgo_search/compat.py:66: UserWarning: parameter max_results is deprecated\n  warnings.warn(\"parameter max_results is deprecated\")\n\n\n\n\n\n\nsearches = 'f1', 'nascar'\npath = Path('race_series')\nfrom time import sleep\n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f'{o} photo'))\n    sleep(10)  # Pause between searches to avoid over-loading server\n    download_images(dest, urls=search_images(f'{o} old photo'))\n    sleep(10)\n    resize_images(path/o, max_size=400, dest=path/o)\n\nSearching for 'f1 photo'\nSearching for 'f1 old photo'\nSearching for 'nascar photo'\nSearching for 'nascar old photo'\n\n\n\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n2\n\n\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path, bs=32)\n\ndls.show_batch(max_n=6)\n\n\n\n\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(3)\n\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00&lt;00:00, 71.4MB/s]\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.431362\n0.930054\n0.300000\n00:09\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.660739\n0.551799\n0.250000\n00:13\n\n\n1\n0.482893\n0.117785\n0.000000\n00:14\n\n\n2\n0.360152\n0.058476\n0.000000\n00:13\n\n\n\n\n\n\nis_f1,_,probs = learn.predict(PILImage.create('f1.jpg'))\nprint(f\"This is a: {is_f1}.\")\nprint(f\"Probability it's a f1 car: {probs[0]:.4f}\")\n\n\n\n\n\n\n\n\nThis is a: f1.\nProbability it's a f1 car: 0.9282\n\n\n\nlearn.export(\"model.pkl\")"
  },
  {
    "objectID": "posts/cnn-notes/cnn-notes.html",
    "href": "posts/cnn-notes/cnn-notes.html",
    "title": "CNN Notes",
    "section": "",
    "text": "I have been working through Andrew Ng’s Deep Learning Specialisation. Here are some of the notes I took during the Convolutional Neural Networks Course. Hopefully they can be of use to you. Some images are taken from the slides in the course."
  },
  {
    "objectID": "posts/cnn-notes/cnn-notes.html#notes",
    "href": "posts/cnn-notes/cnn-notes.html#notes",
    "title": "CNN Notes",
    "section": "Notes",
    "text": "Notes\n\nFoundations of Convolutional Neural Networks\n\nVertical Edge Detection\nYou have a filter and you ‘convolve’ it with the input image You do an element-wise product with the filter overlaying the input image in different positions. The filter for vertical edge detection typically looks something like this \\[\n\\begin{bmatrix}\n1 & 0 & -1 \\\\\n1 & 0 & -1 \\\\\n1 & 0 & -1 \\\\\n\\end{bmatrix}\n\\]\n\n\nPositive and Negative Vertical Edges\nDistinguishes between light-to-dark and dark-to-light edges\n\n\nLearning to Detect Edges\nModern DL does not explicitly define the filter, but rather sets each of the elements as a weight/parameter that the model can learn itself.\n\n\nPadding\nNormal convolutions have some problems. They do not use much information from the edge pixels and they also shrink the output. You can get around both of these problems by padding the border of the input with zeroes.\n\n\nSize of output\n\\((n+2p-f+1) \\times (n+2p-f+1)\\)\n\n\nValid Convolutions\nNo padding #### Same Convoultions Output size is the same as the input size. You can achieve this if you set \\(f\\) and \\(p\\) to be \\[p = \\frac{f-1}{2}\\] To make this formula work, \\(f\\) is normally odd. This has some advantages such as the filter having a central pixel.\n\n\nStrided Convolutions\nYou can add a ‘stride’ parameter \\(s\\) to the convolution. This means you ‘jump’ the filter over by \\(s\\) pixels each time. The new formula for computing the output size is \\[\\lfloor \\frac{n+2p-f}{s} + 1\\rfloor \\times \\lfloor \\frac{n+2p-f}{s} + 1\\rfloor \\] Where \\(\\lfloor x \\rfloor\\) denotes the ‘floor’ of the expression\n\n\nConvolutions over Volume\nYou can convolve a 3d input with a 3d filter. You put the 3d filter over the 3d input as you would in 2d, and you calculate the element-wise product and use that as the first cell of the output.\nNote: the input volume and filter need to have the same number of channels.\nThe model can learn to detect more complex patterns such as vertical edges in only the red channel for example.\n\n\nMultiple Filters\nYou can have multiple filters per layer If you have \\(n_c'\\) filters, then you will have \\(n_c'\\) outputs, which you stack to form a \\(\\lfloor \\frac{n+2p-f}{s} + 1\\rfloor \\times \\lfloor \\frac{n+2p-f}{s} + 1\\rfloor \\times n_c'\\) output\n\n\nOne Layer of a CNN\n\n\n\nimage.png\n\n\n\n\nMulti-Layer CNN\nAs you get deeper in the NN, you reduce height and width, while increasing the number of channels.\n\n\nPooling Layers: Max Pooling\nYou get the max of the corresponding region  Max pooling has hyperparameters \\(f\\) and \\(s\\), but it does not have any weights to learn.\nThe same formula for computing the size of the output still works here: \\[\\lfloor \\frac{n+2p-f}{s} + 1\\rfloor \\times \\lfloor \\frac{n+2p-f}{s} + 1\\rfloor \\times n_c\\]\n\n\nAverage Pooling\nThis is the same as max pooling, but taking the average of the region instead of the max.\nNote: padding is very rarely used in either max or average pooling\n\n\nConvolutional Neural Network Example\n Normally, we put one or more convolutional layer (CONV) followed by a pooling layer (POOL), and repeat this pattern multiple times.\nWhen we are done with the convolutional layers, we ‘flatten’ them out and pass them through a few fully connected (FC) layers.\n\n\nWhy Convolutions?\n\nParameter Sharing\nSparcity of Connections\n\nParameter sharing lets learned parameters to be used in multiple parts of the image. This drastically reduces the total number of parameters required in the neural network.\nSparcity of Connections: since every output is calculated on a small section of the input at a time, it will be easier to learn the parameters.\n\n\nPutting it all together\n\nDecide on an architecture\nDecide on a loss function \\(L\\)\nRandomly initialize all the weights and biases\nUse an optimizer (like gradient descent or Adam) to optimize the parameters\n\n\n\n\nMore Complicated CNNs\n\nResidual Blocks (ResNets)\nResidual blocks get around the vanishing/exploding gradients problem in very deep NNs.\nThey copy an activation from one layer and feed it into another layer deeper in the network.\n\n\n\nimage.png\n\n\n\\[a^{[l+2]} = g(z^{[l+2]} + a^{[l]})\\]\nIf you add more and more layers to a traditional FFNN, it can actually increase training error.\nAdding a ResNet layer doesn’t hurt training error because it is very easy for it to learn the identity function.\nOne thing to note, is that \\(a^{[l]}\\) and \\(a^{[l+2]}\\) have to have the same dimension for the addition to work, so a lot of ‘same’ convolutions are used for this reason.\n\n\n1x1 Convolutions\n1x1 convolutions are used to change the number of channels, \\(n_c\\). For example, if you have a \\(28 \\times 28 \\times 192\\) layer input, and want to reduce the number of channels to \\(32\\) then you pass is through a \\(1 \\times 1\\) convolution with \\(32\\) filters\n\n\nInception Layers\nInception layers can have multiple filter sizes and stack the outputs \nThis lets the model learn which of the filters are most useful and learns the correct parameters for them by itself.\nWe can reduce the computational cost of inceptions layers by using a \\(1 \\times 1\\) convolution to calculate the \\(5 \\times 5\\) convolution. \nSo long as you implement the bottleneck layer within reason, you retain a lot of the power of the original \\(5 \\times 5\\) convolution.\nWe can do the same for the \\(3 \\times 3\\) convolution.\nThe inception layer now looks like this. \nAlthough the maxpool layer gives the correct number of channels as output, it’s still significantly higher than the others, so we pass it through a \\(1 \\times 1\\) convolution with a smaller number of filters, for example \\(32\\) so it takes up a more reasonable amount of the total channels.\n\n\nDepthwise Separable Convolution\n You first pass the input into a Depthwise Convolution and then into a Pointwise Convoluion.\n The depthwise convolution works as follows, you convolve each of the input channels with only the corresponding filter channel. The outputs give the outputs of each of the channels of the output.\n For the pointwise convolution, you have \\(n_c'\\) filters of size \\(1 \\times 1 \\times n_c\\). The output is calculated one channel at a time, using the calculated convolution between the input and each of the filters.\nDepthwise separable convolutions tend to be a lot faster than normal convolutions. The ratio of computation time is given by\n\\[\\frac{1}{n_c'} + \\frac{1}{f^2}\\]\n\n\nImage Classification, Localization and Detection\n\n\n\nimage.png\n\n\nImage classification: Detecting presence or absense of an object\nClassification with localization: Detects presence or absence of object and gives its location\nDetection: Gives locations of multiple objects in an image.\n\n\nClassification with localization\nTo train a model to do classification with localization, your training data needs to have images as inputs and a vector \\(y\\) as the label.\n\\[ \\begin{bmatrix}\np_c \\\\\nb_x \\\\\nb_y \\\\\nb_h \\\\\nb_w \\\\\nc_1 \\\\\nc_2 \\\\\nc_3\n\\end{bmatrix}\n\\]\nThe loss function \\(L(\\hat{y}, y)\\) can be defined using the squared error: \\[\nL(\\hat{y}, y) =\n\\begin{cases}\n(y_{\\hat{1}} - y_1)^2 + (y_{\\hat{2}} - y_2)^2 + \\cdots + (y_{\\hat{8}} - y_8)^2 & \\text{if } y_1 = 1, \\\\\n(y_{\\hat{1}} - y_1)^2 & \\text{if } y_1 = 0.\n\\end{cases}\n\\]\n\n\nLandmark Detection\nThe above method can also be used for landmark detection. By selecting multiple landmarks and labelling them with (for example in people’s faces), the model can learn to do landmark detection on new, unseen faces."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dylan's Blog",
    "section": "",
    "text": "CNN Notes\n\n\n\n\n\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n\nSep 9, 2023\n\n\nDylan Gallagher\n\n\n\n\n\n\n  \n\n\n\n\nNASCAR or F1 Car Classifier\n\n\n\n\n\n\n\nProject\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\nDylan Gallagher\n\n\n\n\n\n\nNo matching items"
  }
]